<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-TW">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.0.6" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.0.6">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.0.6">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.0.6">


  <link rel="mask-icon" href="/images/logo.svg?v=6.0.6" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.0.6',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="odie&#39;s whisper">
<meta property="og:url" content="https://odie2630463.github.io/page/2/index.html">
<meta property="og:site_name" content="odie&#39;s whisper">
<meta property="og:locale" content="zh-TW">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="odie&#39;s whisper">






  <link rel="canonical" href="https://odie2630463.github.io/page/2/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>odie's whisper</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-TW">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> 

<div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">odie's whisper</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切換導航欄">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        
          
  <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首頁</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br />關於</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />標籤</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />分類</a>
</li>

      
        
        
          
  <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />歸檔</a>
</li>

      

      
    </ul>
  

  
    

    
    
      
      
    
      
      
    
      
      
    
      
      
    
      
      
    
    

  


  

  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://odie2630463.github.io/2018/07/28/pyro-lda/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="odie">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="odie's whisper">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/28/pyro-lda/" itemprop="url">LDA model use Pyro</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">發表於</span>
              
              <time title="創建於" itemprop="dateCreated datePublished" datetime="2018-07-28T21:21:44+08:00">2018-07-28</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分類於</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/probabilistic-programming/" itemprop="url" rel="index"><span itemprop="name">probabilistic programming</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="http://pyro.ai" target="_blank" rel="noopener">Pyro</a> 是一個由Uber AI Lab所開發的Probabilistic Programming Language(PPL)，用程式語言來描述具有隨機性的程序或是過程；從另一個角度來看，圖機率模型(PGM)是用圖的方式來描述一個機率過程，而PPL則是可以讓你用上程式語言來描述，條件、迴圈等等都可以用上，能描述更為複雜的機率過程</p>
<p>PPL也存在好一陣子了，像是MIT基於LISP所開發的<a href="https://arxiv.org/abs/1206.3255" target="_blank" rel="noopener">Church</a>、<a href="http://webppl.org" target="_blank" rel="noopener">webppl</a>都是，但都偏向是學術性的語言；隨著深度學習也用上了一些機率推斷方法，像是MCMC、Variational inference等，也出現基於Tensorflow、Pytorch框架的PPL，能更好的與深度學習方法結合，並且引入很多有用的機率數學工具</p>
<h2 id="Introduction-of-Pyro"><a href="#Introduction-of-Pyro" class="headerlink" title="Introduction of Pyro"></a>Introduction of Pyro</h2><p>根據官網描述，Pyro有幾個特點</p>
<ul>
<li>Universal: Pyro can represent any computable probability distribution.</li>
<li>Scalable: Pyro scales to large data sets with little overhead.</li>
<li>Minimal: Pyro is implemented with a small core of powerful, composable abstractions.</li>
<li>Flexible: Pyro aims for automation when you want it, control when you need it. </li>
</ul>
<p>基於Pytorch開發，所以深度學習那些當然都可以整合一起用上，並且語法非常的Pythonic，也用上了很多高階特性，例如context manager，機率推斷演算法主要提供Stochastic Variational Inference(SVI)，可以用上SGD來訓練模型，整體上的確以最小需要、高彈性、透明為主要設計，相較於基於Tensorflow的<a href="http://edwardlib.org" target="_blank" rel="noopener">Edward</a>、<a href="https://github.com/tensorflow/probability" target="_blank" rel="noopener">Tensorflow_probability</a>，提供不只VI相關演算法，還有很多MCMC相關的，或是<a href="https://odie2630463.github.io/2018/07/11/flow/">前一篇提到的NF</a>，更多工具可以使用，但也相對不是那麼容易上手；不過結合更多機率特性是趨勢之一，這兩個主流的Deep PPL都很值得關注與學習。</p>
<h2 id="Implement-LDA-model"><a href="#Implement-LDA-model" class="headerlink" title="Implement LDA model"></a>Implement LDA model</h2><p>以上介紹了Pyro，接著就來實作Latent Dirichlet allocation(LDA)模型，常用文本主題分析，且為圖機率模型的經典模型！接著用Pyro與SVI來實作並求解參數，本文並不會從頭介紹Pyro基礎，建議可以先看<a href="http://pyro.ai/examples/index.html" target="_blank" rel="noopener">官網教學</a>，程式碼我放在<a href="https://colab.research.google.com/drive/11NIOR7Ix2pxkQPw_m0zZaai9Fi935-l7" target="_blank" rel="noopener">Colab</a>，大家可以直接跑看看</p>
<h4 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h4><p>我們就直接從模型看起，先來看一下LDA式子長怎樣，$\phi$指每個字對應個主題的機率分佈，$\theta_d$指每一文件對應主題的機率分佈，$z$指每個字被分配到的主題，式子可以寫成：</p>
<p>$$<br>p(\phi,\theta,z,w) = p(\phi) \prod_{d=1}^D \prod_{n=1}^{N_d} p(z_{dn}|\theta_d)p(w_{dn}|z_{dn},\phi)<br>$$</p>
<p>式子對應Pyro程式碼如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@pyro.poutine.broadcast</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(data)</span>:</span></span><br><span class="line">  phi = pyro.sample(<span class="string">"phi"</span>,dist.Dirichlet(torch.ones([K, V])).independent(<span class="number">1</span>))</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span> d <span class="keyword">in</span> pyro.irange(<span class="string">"documents"</span>, D):</span><br><span class="line">    theta_d = pyro.sample(<span class="string">"theta_%d"</span>%d, dist.Dirichlet(torch.ones([K])))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> pyro.iarange(<span class="string">"words_%d"</span>%d, N[d]):</span><br><span class="line">      z = pyro.sample(<span class="string">"z_%d"</span>%d, dist.Categorical(theta_d))</span><br><span class="line">      pyro.sample(<span class="string">"w_%d"</span>%d, dist.Categorical(phi[z]), obs=data[d])</span><br></pre></td></tr></table></figure>

<h4 id="Guide"><a href="#Guide" class="headerlink" title="Guide"></a>Guide</h4><p>$w$是已知觀測變量，想要估計的隱變量有$\phi,\theta,z$，我們可以用上Variational inference方法來最大化資料似然$\log p(w)$，並用另一個<strong>guide</strong>來近似$p(\phi,\theta,z|w)$，有關於VI相關會在後續寫個幾篇好好來講講</p>
<p>$$<br>q(\phi,\theta,z) = q(\phi) \prod_{d=1}^D q(\theta_d)\prod_{n=1}^{N_d} q(z_{dn})<br>$$</p>
<p>式子對應Pyro程式碼如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@pyro.poutine.broadcast</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">guide</span><span class="params">(data)</span>:</span></span><br><span class="line">  beta_q = pyro.param(<span class="string">"beta_q"</span>, torch.ones([K, V]),constraint=constraints.positive)</span><br><span class="line">  phi_q = pyro.sample(<span class="string">"phi"</span>,dist.Dirichlet(beta_q).independent(<span class="number">1</span>))</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span> d <span class="keyword">in</span> pyro.irange(<span class="string">"documents"</span>, D):</span><br><span class="line">    alpha_q = pyro.param(<span class="string">"alpha_q_%d"</span>%d, torch.ones([K]),constraint=constraints.positive)</span><br><span class="line">    q_theta_d = pyro.sample(<span class="string">"theta_%d"</span>%d, dist.Dirichlet(alpha_q))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> pyro.iarange(<span class="string">"words_%d"</span>%d, N[d]):</span><br><span class="line">      q_i = pyro.param(<span class="string">"q_%d"</span>%d, torch.randn([N[d], K]).exp(),</span><br><span class="line">                       constraint=constraints.simplex)</span><br><span class="line">      pyro.sample(<span class="string">"z_%d"</span>%d, dist.Categorical(q_i))</span><br></pre></td></tr></table></figure>

<p>注意<strong>guide</strong>裡面有參數是需要訓練的，比如每一個字所對應的$z_dn$，我們都用一個Categorical分佈來代表主題，都有參數需要訓練，很直接去估文件裡的每個字代表什麼主題</p>
<h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>最後用SVI去最大化ELBO</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">adam_params = &#123;<span class="string">"lr"</span>: <span class="number">0.01</span>, <span class="string">"betas"</span>: (<span class="number">0.90</span>, <span class="number">0.999</span>)&#125;</span><br><span class="line">optimizer = Adam(adam_params)</span><br><span class="line"></span><br><span class="line">svi = SVI(model, config_enumerate(guide, <span class="string">'parallel'</span>), optimizer, loss=TraceEnum_ELBO(max_iarange_nesting=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3000</span>):</span><br><span class="line">  loss = svi.step(data)</span><br></pre></td></tr></table></figure>

<p>TraceEnum_ELBO適用於有離散變量需要估計的時候，會直接去做enumerate，不然單用Trace_ELBO遇到離散變量效果都很差</p>
<h4 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">pyro.param(<span class="string">'q_2'</span>)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">tensor([[<span class="number">0.8072</span>, <span class="number">0.0559</span>, <span class="number">0.0606</span>, <span class="number">0.0543</span>, <span class="number">0.0220</span>],</span><br><span class="line">        [<span class="number">0.9249</span>, <span class="number">0.0258</span>, <span class="number">0.0146</span>, <span class="number">0.0242</span>, <span class="number">0.0104</span>],</span><br><span class="line">        [<span class="number">0.9250</span>, <span class="number">0.0258</span>, <span class="number">0.0145</span>, <span class="number">0.0243</span>, <span class="number">0.0105</span>],</span><br><span class="line">        [<span class="number">0.3842</span>, <span class="number">0.0408</span>, <span class="number">0.5215</span>, <span class="number">0.0381</span>, <span class="number">0.0153</span>],</span><br><span class="line">        [<span class="number">0.9406</span>, <span class="number">0.0206</span>, <span class="number">0.0110</span>, <span class="number">0.0193</span>, <span class="number">0.0085</span>],</span><br><span class="line">        [<span class="number">0.9406</span>, <span class="number">0.0206</span>, <span class="number">0.0111</span>, <span class="number">0.0193</span>, <span class="number">0.0084</span>],</span><br><span class="line">        [<span class="number">0.9406</span>, <span class="number">0.0204</span>, <span class="number">0.0110</span>, <span class="number">0.0192</span>, <span class="number">0.0088</span>],</span><br><span class="line">        [<span class="number">0.9406</span>, <span class="number">0.0205</span>, <span class="number">0.0111</span>, <span class="number">0.0193</span>, <span class="number">0.0085</span>],</span><br><span class="line">        [<span class="number">0.9250</span>, <span class="number">0.0258</span>, <span class="number">0.0145</span>, <span class="number">0.0242</span>, <span class="number">0.0105</span>]], grad_fn=&lt;DivBackward1&gt;)</span><br><span class="line">        </span><br><span class="line">z[<span class="number">2</span>]</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<p>看一下第二筆文件裡的字，估計主題效果還不錯</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">phi[<span class="number">0</span>]</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">tensor([<span class="number">1.1714e-03</span>, <span class="number">7.1110e-01</span>, <span class="number">1.4448e-03</span>, <span class="number">1.8015e-03</span>, <span class="number">1.0734e-01</span>, <span class="number">2.6613e-06</span>,</span><br><span class="line">        <span class="number">1.1921e-07</span>, <span class="number">3.6218e-03</span>, <span class="number">9.7121e-02</span>, <span class="number">1.1921e-07</span>, <span class="number">8.0717e-03</span>, <span class="number">7.9834e-07</span>,</span><br><span class="line">        <span class="number">6.8322e-02</span>, <span class="number">1.1921e-07</span>, <span class="number">1.1921e-07</span>])</span><br><span class="line"></span><br><span class="line">dist.Dirichlet(pyro.param(<span class="string">'beta_q'</span>)).sample()[<span class="number">0</span>]</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">tensor([<span class="number">0.0089</span>, <span class="number">0.4932</span>, <span class="number">0.0083</span>, <span class="number">0.0549</span>, <span class="number">0.1486</span>, <span class="number">0.0355</span>, <span class="number">0.0272</span>, <span class="number">0.0291</span>, <span class="number">0.0509</span>,</span><br><span class="line">        <span class="number">0.0098</span>, <span class="number">0.0045</span>, <span class="number">0.0631</span>, <span class="number">0.0331</span>, <span class="number">0.0021</span>, <span class="number">0.0308</span>])</span><br></pre></td></tr></table></figure>

<p>來看第1個主題對應字的機率，都是第2個字的機率最高，代表主題1最相關的字為第2個，雖然分佈可能還有點差異，但效果也還不差</p>
<h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>深度學習結合機率方法是趨勢，這些PPL框架能更快投入研究與產品，後續會再分享一些Pyro應用與心得</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://odie2630463.github.io/2018/07/11/flow/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="odie">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="odie's whisper">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/11/flow/" itemprop="url">Normalizing Flows</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">發表於</span>
              
              <time title="創建於" itemprop="dateCreated datePublished" datetime="2018-07-11T15:13:46+08:00">2018-07-11</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分類於</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/flow/" itemprop="url" rel="index"><span itemprop="name">flow</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>機率分佈估計一直是機器學習中的核心問題，不論是分類問題估計條件機率分佈$p(y|x)$，或是建立生成模型$p(x,z)$，我們都在從資料中建立與學習出一個機率分佈</p>
<p>基本上機率分佈能做到兩件事：</p>
<ul>
<li>評估機率值，計算$p(x)$，給樣本空間的點都要能得出機率值</li>
<li>抽樣，計算$x \sim p(x)$，要能依機率密度來抽樣出樣本</li>
</ul>
<p>近年來，深度學習能有效率的近似高維度的複雜函數，而且還有完整的訓練框架，再加上機率圖模型概念，能表示很多複雜的機率分佈，但雖然能訓練模型來估計複雜分佈，也不一定能做到上面說的兩件事，比如：GAN很容易生成新影像，但卻無法給一張影像估計其機率值</p>
<p>Normalizing Flows是一種能將機率分佈轉換到另一個分佈的數學工具，例如把簡單的高斯分佈轉換至另一個複雜的機率分佈上，且依然能有效率評估機率值與抽樣，最近很多論文也開始結合這類的方法，例如：</p>
<ul>
<li><a href="https://arxiv.org/abs/1606.04934" target="_blank" rel="noopener">Improving Variational Inference with Inverse Autoregressive Flow</a></li>
<li><a href="https://arxiv.org/abs/1705.07057" target="_blank" rel="noopener">Masked Autoregressive Flow for Density Estimation</a></li>
</ul>
<p>還有最近由OpenAI剛發佈的<a href="https://arxiv.org/abs/1807.03039" target="_blank" rel="noopener">Glow: Generative Flow with Invertible 1x1 Convolutions</a>，基於Normalizing Flows的一種生成模型，也能生成高品質影像與表徵解構</p>
<h2 id="Normalizing-Flows"><a href="#Normalizing-Flows" class="headerlink" title="Normalizing Flows"></a>Normalizing Flows</h2><p>假設隨機變量$x \in \mathbb{R}^d$且$x \sim p(x)$，與一個可逆函數$f:\mathbb{R}^d \mapsto \mathbb{R}^d$，我們用$f$來轉換$p(x)$，可以得到另一個經過轉換的隨機變量$y=f(x)$與其機率分佈：</p>
<p>$<br>p(y) = p(x) \left|<br>    \mathrm{det} \frac{<br>      \partial f^{-1}<br>    }{<br>      \partial x<br>    }<br>  \right|<br>  = p(x) \left|<br>    \mathrm{det} \frac{<br>      \partial f<br>    }{<br>      \partial x<br>    }<br>  \right| ^{-1} \tag{1}<br>$</p>
<p>行列式直觀上是面積或是體積的<strong>縮放比例</strong>，機率為機率密度函數底下的面積，轉換函數$f$可是視為一種面積的縮放變形，所以$p(y)$由轉換前$p(x)$底下的面積，乘上行列式表示的面積縮放比例，因為是計算$f^{-1}$，這個面積縮放的比例係指$y$對於$x$影響，意思是說，給$y$來反推回去$x$，並調整$p(x)$底下的面積大小，但也可以計算向前過程的行列式來取倒數。</p>
<p>假設轉換由多個轉換函數$f_1 \dots f_K$組成，$z_k$為轉換過程的中間變量，最後得到的隨機變量$y = f_K \circ \dots\circ f_1(x)$，計算$\log p(y)$為：</p>
<p>$<br>\log p(y) = \log p(x) - \sum_{k=1}^K<br>  \log<br>    \left|<br>    \mathrm{det} \frac{<br>      \partial f_k<br>    }{<br>      \partial \mathbf{x}_{k-1}<br>    }<br>  \right| \tag{2}<br>$</p>
<p>綜上所述，NF有兩種計算，<strong>第一是向前抽樣計算</strong>，從$x$計算$y = f(x)$，<strong>第二是向後機率機算</strong>，給一個$y$要一步步往後做逆運算，計算轉換的行列式總和，最後求出$x$。</p>
<p>計算行列式最差時候需要<a href="https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations" target="_blank" rel="noopener">$O(n^3)$</a>，所以就需要設計比較容易計算行列式的轉換，例如 Masked Autoregressive Flow、Inversed Autoregressive Flow、RealNVP等等模型，有用到Autoregressive的特性，所以行列式會很容易計算，這些方法會在後續的文章中介紹。</p>
<h2 id="Training-and-sampling"><a href="#Training-and-sampling" class="headerlink" title="Training and sampling"></a>Training and sampling</h2><p><strong>向後機率計算</strong>，我們可以直接對$\log p(y)$最大化機率似然，給$y$透過一系列反函數得到$x$與行列式總和</p>
<p>$$x = f_1^{-1}  \circ \dots \circ f_K^{-1}(y)$$</p>
<p>最後計算$\log p(x)$減去轉換過程行列式總和(如式子2)，就可以算出$\log p(y)$，當然也就能用上SGD等優化方法，一系列轉換函數就當成一層層神經網路來訓練，只是這裡要求要能計算反函數</p>
<p><strong>向前抽樣計算</strong>，出新樣本很直觀，就從$p(x)$抽樣出$x$，經過轉換即可得到$y$，而且還能順便得到$p(y)$</p>
<p>$$y = f_K  \circ \dots \circ f_1(x)$$</p>
<hr>
<p>如果對NF有興趣，參考資料都很值得一看！</p>
<p>之後會放上一個實作例子，把簡單機率分佈轉換至一個複雜分佈，還有關於最近NF發展的論文的閱讀心得</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a href="https://blog.evjang.com/2018/01/nf1.html" target="_blank" rel="noopener">Normalizing Flows Tutorial by Eric Jang</a></li>
<li><a href="http://akosiorek.github.io/ml/2018/04/03/norm_flows.html" target="_blank" rel="noopener">Normalizing Flows by Adam Kosiorek</a></li>
<li><a href="https://www.slideshare.net/daynap1204/improving-variational-inference-with-inverse-autoregressive-flow-71213560" target="_blank" rel="noopener">Improving Variational Inference with Inverse Autoregressive Flow</a></li>
<li><a href="http://bjlkeng.github.io/posts/variational-autoencoders-with-inverse-autoregressive-flows/" target="_blank" rel="noopener">Variational Autoencoders with Inverse Autoregressive Flows</a></li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://odie2630463.github.io/2018/04/15/sketchrnn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="odie">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="odie's whisper">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/15/sketchrnn/" itemprop="url">SketchRNN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">發表於</span>
              
              <time title="創建於" itemprop="dateCreated datePublished" datetime="2018-04-15T23:19:54+08:00">2018-04-15</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分類於</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>之前Google有一個有趣的小遊戲<a href="https://quickdraw.withgoogle.com" target="_blank" rel="noopener">Quickdraw</a>，給你一個題目要你20秒內畫出來，當然除了讓你打發時間外，大家手繪大作也變成機器的精神糧食，拿來學習了XD</p>
<p><img src="/media/7FB391C9-1C05-4051-88FE-80FF2961ED8F.png" alt="7FB391C9-1C05-4051-88FE-80FF2961ED8F"></p>
<p>SketchRNN是一個向量軌跡的生成模型，這篇<a href="https://research.googleblog.com/2017/04/teaching-machines-to-draw.html" target="_blank" rel="noopener">部落格文章</a>可以看到很多有趣的結果，詳細的模型架構與訓練過程可以看這一篇<a href="https://arxiv.org/abs/1704.03477" target="_blank" rel="noopener">論文</a></p>
<p><img src="/media/15231614597425.png" alt=""></p>
<p>整個模型可分為3個部分：</p>
<ul>
<li>Seq2seq autoencoder + Gaussian mixture model</li>
<li>Variational inference</li>
</ul>
<h2 id="Seq2seq-autoencoder-Gaussian-mixture-model"><a href="#Seq2seq-autoencoder-Gaussian-mixture-model" class="headerlink" title="Seq2seq autoencoder + Gaussian mixture model"></a>Seq2seq autoencoder + Gaussian mixture model</h2><p>Seq2seq+GMM這是很經典的序列至序列架構，最早是出現在這一篇<a href="https://arxiv.org/pdf/1308.0850.pdf" target="_blank" rel="noopener">Generating Sequences With Recurrent Neural Networks</a>，並且有一個<a href="https://www.cs.toronto.edu/~graves/handwriting.html" target="_blank" rel="noopener">手寫筆跡產生</a>的應用，也算是這個塗鴉的前身之作，作者是Alex Graves，他可是Hinton的博士生喔！</p>
<p>架構上是一個序列至序列模型，$encoder$是雙向的LSTM，每個時間點吃入筆跡$x,y$實數座標值與下筆狀態類別，而$decoder$是單向的LSTM，每個時間點輸出下一個時間點的筆跡座標與下筆狀態</p>
<p>下筆狀態是離散的類別，這個很容易處理，只是簡單的分類問題，用個$softmax$就搞定了，輸出就是離散類別的機率分佈，但是連續資料我們要怎麼處理呢？怎麼建立連續的機率分佈呢？</p>
<p>我們常用高斯分佈是來作為連續變量的機率分佈，每個時間點可以輸出$\mu,\sigma$來建立高斯分佈，但對於複雜的資料，只用一個高斯分佈可能太過簡單，無法很好的捕捉資料的分佈，例如：筆跡座標可能有蠻大的變異性，很難說每個時間點就只用一個高斯就能代表，畢盡畫圖很隨性的呀～</p>
<p>所以用上GMM來增加輸出的機率分佈複雜度，由多個高斯分佈組合起來，形成一個複雜的機率分佈，如圖，用上3個不同參數的高斯分佈，就能組合出紅色的這個複雜分佈</p>
<p><img src="/media/15231633369052.jpg" alt=""></p>
<p>每個時間點模型給出的預測$\hat{y}$為公式17，分別為是否停止、GMM的參數等，就可以直接做MLE訓練</p>
<p><img src="/media/BD8784F1-5355-4BE2-A726-BBB86271B739.png" alt="BD8784F1-5355-4BE2-A726-BBB86271B739"></p>
<h2 id="Variational-inference"><a href="#Variational-inference" class="headerlink" title="Variational inference"></a>Variational inference</h2><p>VAE這個東西是一個非常熱門的生成模型，下次有機會再開一篇來講講，如果想簡單了解，可以直接看<a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/" target="_blank" rel="noopener">這裡</a>，直觀上SketchRNN透過$encoder$把序列壓縮到一個連續空間中，這個連續空間為高斯分佈$N(0,1)$，而$decoder$要能還原回原本的軌跡，但我們能從連續空間中抽樣出新的點，由$decoder$產生軌跡，就是產生新的塗鴉囉！</p>
<h2 id="Implement-and-Tricks"><a href="#Implement-and-Tricks" class="headerlink" title="Implement and Tricks"></a>Implement and Tricks</h2><p>我的<a href="https://github.com/odie2630463/sketch_rnn" target="_blank" rel="noopener">實作版本</a>，以Pytorch實作</p>
<h3 id="Data-processing-amp-Training"><a href="#Data-processing-amp-Training" class="headerlink" title="Data processing &amp; Training"></a>Data processing &amp; Training</h3><ul>
<li><a href="https://github.com/googlecreativelab/quickdraw-dataset" target="_blank" rel="noopener">quickdraw-dataset</a>有約300多種類別的塗鴉資料可以下載</li>
<li>軌跡都是正整數，要先剔除一些異常資料，再做縮放正規化，這部分可以參考實作</li>
<li>Google實作還有做一些data augmentation，有些類別資料很少，但我實作裡沒有</li>
<li>訓練就跟一般seq2seq差不多，而且蠻快的</li>
<li>產生新樣本時，不論是直接從高斯抽樣，或是從給定的序列去還原，有一個部分要特別注意，就是tempture，調整tempture會改變輸出的機率分佈，低的溫度會使分佈更為集中，高的反之亦然，溫度低筆觸就不會到處亂飄，因為機率分佈集中了，比較容易畫出像樣的圖，而溫度高，分佈就比較均勻，筆觸就容易到處飄，就不容易畫出好的圖 </li>
</ul>
<p><img src="/media/15233779330198.jpg" alt=""></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a href="https://research.googleblog.com/2017/04/teaching-machines-to-draw.html" target="_blank" rel="noopener">Teaching Machines to Draw</a></li>
<li><a href="https://arxiv.org/abs/1704.03477" target="_blank" rel="noopener">A Neural Representation of Sketch Drawings</a></li>
<li><a href="https://github.com/tensorflow/magenta/tree/master/magenta/models/sketch_rnn" target="_blank" rel="noopener">magenta/models/sketch_rnn</a></li>
<li><a href="https://github.com/alexis-jacq/Pytorch-Sketch-RNN" target="_blank" rel="noopener">alexis-jacq/Pytorch-Sketch-RNN</a></li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://odie2630463.github.io/2018/04/01/wavenet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="odie">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="odie's whisper">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/01/wavenet/" itemprop="url">Ｗavenet</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">發表於</span>
              
              <time title="創建於" itemprop="dateCreated datePublished" datetime="2018-04-01T20:20:53+08:00">2018-04-01</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分類於</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" target="_blank" rel="noopener">WaveNet: A Generative Model for Raw Audio</a> 是 DeepMind所提出的一種用於聲音的生成模型，例如最近發布的<a href="https://cloud.google.com/text-to-speech/" target="_blank" rel="noopener">CLOUD TEXT-TO-SPEECH</a>用的模型架構就是以這個為基礎，還有<a href="https://magenta.tensorflow.org/nsynth-instrument" target="_blank" rel="noopener">Making a Neural Synthesizer Instrument</a>裡面都有用上類似WaveNet的結構</p>
<h2 id="Autoregressive-model"><a href="#Autoregressive-model" class="headerlink" title="Autoregressive model"></a>Autoregressive model</h2><p>統計上常用來處理時間序列的方法，用過去歷史資料 $x_{1:t-1}$來預測現在$x_t$，可以看成過去的自己對現在的自己做回歸，所以被稱為自回歸模型</p>
<p>本模型也是用過去的聲音訊號來預測現在的訊號點，這樣的缺點是要生成新樣本的時候效率很差，因為資料有序列關係，總是要等待前面產生，才可以依序產生新的點，算是這類模型的問題</p>
<h2 id="Fully-observed-model"><a href="#Fully-observed-model" class="headerlink" title="Fully observed model"></a>Fully observed model</h2><p>本模型沒有任何的Latent variable，沒有需要估計或是近似的未知變量，任何資訊都是完全可見的，都是從過去歷史資料來的，這樣的模型架構上更為直接簡單，不需要處理隱變量問題，用上繁瑣的近似演算法等等，而可以直接做MLE訓練</p>
<h2 id="Implement-and-Tricks"><a href="#Implement-and-Tricks" class="headerlink" title="Implement and Tricks"></a>Implement and Tricks</h2><p>我的<a href="https://github.com/odie2630463/WaveNet" target="_blank" rel="noopener">實作版本</a>，以Pytorch實作，僅有針對單一人語音訓練，沒有做多人訓練或是TTS等，但實作上相對透明簡單，可以比較深入看看實作過程</p>
<h3 id="Causal-amp-Dilated-Conv1d"><a href="#Causal-amp-Dilated-Conv1d" class="headerlink" title="Causal &amp; Dilated Conv1d"></a>Causal &amp; Dilated Conv1d</h3><p><img src="/media/15228972877719.gif" alt=""><br>圖片來源：<a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" target="_blank" rel="noopener">WaveNet: A Generative Model for Raw Audio</a></p>
<ul>
<li>Causal:不能用到未來的資訊，例如在實作 PixelCNN時，要把Kernel的一部分mask起來，但是在本實作1D conv時，Kernel直接是2，意思為要預測$x_{t}$時，僅用到$x_{t-2}$與$x_{t-1}$，不過要記得把最後一個時間去掉</li>
<li>Dilated: 讓Kernel看遠一點，不要只看鄰居，如果Dilated設定大一些時，就會看$x_{t}$與$x_{t-N}$，大幅增加視野域，我猜音訊鄰近的時間點值都差不多，不如看遠一點可能會有比較多變化，提供模型更多有用訊息；建議參考<a href="http://sergeiturukin.com/2017/03/02/wavenet.html" target="_blank" rel="noopener">這篇</a>來了解Dilated Conv</li>
</ul>
<h3 id="Residual-amp-Stack-block"><a href="#Residual-amp-Stack-block" class="headerlink" title="Residual &amp; Stack block"></a>Residual &amp; Stack block</h3><p><img src="/media/15228552689393.png" alt=""><br>圖片來源：<a href="http://musyoku.github.io/2016/09/18/wavenet-a-generative-model-for-raw-audio/" target="_blank" rel="noopener">ご注文は機械学習ですか</a></p>
<p><img src="/media/15228902504332.png" alt=""><br>圖片來源：<a href="https://arxiv.org/abs/1609.03499" target="_blank" rel="noopener">WaveNet: A Generative Model for Raw Audio</a></p>
<ul>
<li>模型以多個Block堆疊在一起，每個Block含有多個Dilated Conv，並且加入Residual link來幫助梯度傳遞</li>
<li>把每一個Block的輸出匯集並加起來，每個Block計算視野不同，越往上層越大，代表資料不同的resolution，最後加總在經過轉換輸出得到下一個時間點的預測</li>
</ul>
<h3 id="Data-processing-amp-Training"><a href="#Data-processing-amp-Training" class="headerlink" title="Data processing &amp; Training"></a>Data processing &amp; Training</h3><ul>
<li><a href="http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html" target="_blank" rel="noopener">VCTK</a>資料集有多人的語音與文字內容，可以用來訓練TTS系統</li>
<li>語音資料處理我用上<a href="https://github.com/pytorch/audio" target="_blank" rel="noopener">torchaudio</a>、<a href="https://librosa.github.io/librosa/" target="_blank" rel="noopener">librosa</a>，但不知道為什麼用兩個套件讀出來的有點差異，我最後用librosa來讀取音訊，還有把<strong>無聲部分去掉</strong>，這個步驟非常重要！不然訓練到最後只會一直產生靜音，再用torchaudio做Mulaw encoding</li>
<li>訓練batch size為1，一次給一段語音來訓練，基本上因為訓練資料長度差異很大，要做padding很浪費與不好做，再且顯卡記憶體也不夠大，所以一次訓練一個語音檔是合理的做法</li>
<li>訓練大概要2天，而且頗慢的，建議做learning rate scheduling從0.001往下到0.00001，我訓練在p225這個語音上，最後得到loss約2左右，可以得到還可以的結果</li>
<li>計算視野其實蠻簡單，就餵給模型資料來測試看多長的資料，模型剛好輸出為1，就是模型的視野，代表要預測下一個時間點，需要往前看多長的資料</li>
<li>產生新樣本非常慢的，請耐心等待，所以這一篇頂多只能說是開一個研究開頭，而且也無法直接做到TTS或其他應用，而是一個好的音訊產生網路結構，後續很多聲音產生都是基於此</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a href="http://sergeiturukin.com/2017/03/02/wavenet.html" target="_blank" rel="noopener">Sergei Turukin blog</a></li>
<li><a href="https://lirnli.wordpress.com/2017/10/16/pytorch-wavenet/" target="_blank" rel="noopener">winter plum blog</a></li>
<li><a href="http://musyoku.github.io/2016/09/18/wavenet-a-generative-model-for-raw-audio/" target="_blank" rel="noopener">ご注文は機械学習ですか</a></li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一頁"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">odie</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">14</span>
                    <span class="site-state-item-name">文章</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">7</span>
                    <span class="site-state-item-name">分類</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">6</span>
                    <span class="site-state-item-name">標籤</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love" id="animate"> 
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">odie</span>

  

  
</div>




  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 強力驅動</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主題 &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Muse</a> v6.0.6</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.0.6"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.0.6"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.0.6"></script>



  



	





  





  










  





  

  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    
  


  
  

  

  

  

  

</body>
</html>
